{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install Required Libraries**"
      ],
      "metadata": {
        "id": "xkoqSSM0rcWF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "id": "cOskXY15ouo4"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dependencies**"
      ],
      "metadata": {
        "id": "fdBOqyHMrpNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import re\n",
        "import random\n",
        "import schedule\n",
        "import time"
      ],
      "metadata": {
        "id": "B9N3xRNMro1v"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating BS4 Functions for scrapping**\n",
        "\n",
        "Movies are cateogirsed into seven and each category is processed by indivitual team members.\n",
        "\n",
        "- category 1: 1940 to 1980 200 movie listing with rating=10000\n",
        "- category 2: 2020 to 2021 200 movie listing with rating=20000\n",
        "- category 3: 2000 to 2021 200 movie listing with rating=60000\n",
        "- category 4: 1940 to 1980 200 movie listing with rating=10000\n",
        "- category 5: 1980 to 2019 200 movie listing with rating=500000\n",
        "- category 6: 1980 to 2019 200 movie listing with rating=80000\n",
        "- category 7: 2005 to 2010 200 movie listing with rating=5000"
      ],
      "metadata": {
        "id": "90CIBqBvsAUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample files are put into Data_scrapped folder."
      ],
      "metadata": {
        "id": "n98TQN7xse3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://www.imdb.com/search/title/?title_type=feature&release_date=2009-01-01,2011-12-31&num_votes=2000,&count=200\" #2000 - 2021 6000votes filter 200 titles (imdb not letting to filter >200 titles in a go)\n",
        "\n",
        "def getSoup(url):\n",
        "    \"\"\"\n",
        "    Utility function this get soup function will fetch the above url which stored in url var.\n",
        "    \"\"\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    return soup\n",
        "\n",
        "def getReviews(soup):\n",
        "    '''Function returns all reviews including positive and negative..'''\n",
        "\n",
        "    # get a list of user ratings\n",
        "    user_review_ratings = [tag.previous_element for tag in soup.find_all('span', attrs={'class': 'point-scale'})]\n",
        "\n",
        "    # get the review tags\n",
        "    user_review_list = soup.find_all('a', attrs={'class':'title'})\n",
        "    ans = []\n",
        "    for i in range(5):\n",
        "        ans.append(user_review_list[random.randint(0, len(user_review_list) - 1)])\n",
        "    links = [\"https://www.imdb.com\" + tag['href'] for tag in ans]\n",
        "    return links\n",
        "\n",
        "def getReviewText(review_url):\n",
        "    '''Returns the user review text given the review url.'''\n",
        "    # get the review_url's soup\n",
        "    soup = getSoup(review_url)\n",
        "    # find div tags with class text show-more__control\n",
        "    tag = soup.find('div', attrs={'class': 'text show-more__control'})\n",
        "    return tag.getText()\n",
        "\n",
        "def getMovieTitle(review_url):\n",
        "    '''Returns the movie title from the review url.'''\n",
        "    # get the review_url's soup\n",
        "    soup = getSoup(review_url)\n",
        "    # find h1 tag\n",
        "    tag = soup.find('h1')\n",
        "    return list(tag.children)[1].getText()\n",
        "\n",
        "def scrape_reviews():\n",
        "    url = \"https://www.imdb.com/search/title/?title_type=feature&release_date=2009-01-01,2011-12-31&num_votes=2000,&count=200\"\n",
        "    movies_soup = getSoup(url)\n",
        "\n",
        "    movie_tags = movies_soup.find_all('a', attrs={'class': None})\n",
        "    movie_tags = [tag.attrs['href'] for tag in movie_tags if tag.attrs['href'].startswith('/title') & tag.attrs['href'].endswith('/')]\n",
        "    movie_tags = list(dict.fromkeys(movie_tags))\n",
        "\n",
        "    base_url = \"https://www.imdb.com\"\n",
        "    movie_links = [base_url + tag + 'reviews' for tag in movie_tags]\n",
        "    movie_soups = [getSoup(link) for link in movie_links]\n",
        "\n",
        "    movie_review_list = [getReviews(movie_soup) for movie_soup in movie_soups]\n",
        "    movie_review_list = list(itertools.chain(*movie_review_list))\n",
        "\n",
        "    review_texts = [getReviewText(url) for url in movie_review_list]\n",
        "    movie_titles = [getMovieTitle(url) for url in movie_review_list]\n",
        "\n",
        "    df = pd.DataFrame({'user_review': review_texts})\n",
        "    text_list = [m for m in df['user_review']]\n",
        "    text_list_length = [len(m.split()) for m in text_list]\n",
        "    df['length'] = text_list_length\n",
        "    df = df[df['length'] < 250]\n",
        "    df.drop('length', axis=1, inplace=True)\n",
        "    df.to_csv('data_scrapped/data.csv', index=False)\n",
        "\n",
        "    import csv\n",
        "    with open(\"data_scrapped/data.csv\", \"r\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        rownumber = 2639\n",
        "        for row in reader:\n",
        "            g = open(f\"data_scrapped/{rownumber}.txt\", \"w\")\n",
        "            g.write(str(row))\n",
        "            rownumber += 1\n",
        "            g.close()\n",
        "\n",
        "def job():\n",
        "    print(\"Starting the scraping job...\")\n",
        "    scrape_reviews()\n",
        "    print(\"Scraping job finished.\")\n",
        "\n",
        "# Schedule the job every day at a specific time (e.g., 02:00 AM)\n",
        "schedule.every().day.at(\"02:00\").do(job)\n",
        "\n",
        "print(\"Scheduler is running... Press Ctrl+C to stop.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnQ5Hiz3r_Aw",
        "outputId": "8788064e-f3b6-458b-fb14-37b485058236"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scheduler is running... Press Ctrl+C to stop.\n"
          ]
        }
      ]
    }
  ]
}